# -*- coding: utf-8 -*-
"""Weather_Prediction_Delhi_Hourly_and_Daily.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1veZcSoIJ0L6_bfoaP3ZNyi7RQm-sFd8m

Extracting Hourly Features: Resample the data to create hourly features.
Handling Missing Values: Clean the dataset by removing rows with missing values.
Scaling Data: Min-Max scaling for the features.
Training Multiple Models: Train various regression models and evaluate their performance.
Hyperparameter Tuning: Perform hyperparameter tuning for selected models.
LSTM Model: Include an LSTM model for time series prediction.
Visualization: Plot results and correlation matrices.
"""

def extract_hourly_features(data):
  """Extract hourly features from the dataset."""
  # Resample to hourly frequency and aggregate only numeric columns
  numeric_columns = data.select_dtypes(include=[np.number]).columns.tolist()
  hourly_features = data.resample('H')[numeric_columns].agg(['mean', 'sum', 'max', 'min']).reset_index()
  hourly_features.columns = ['date'] + [f"{col[0]}_{col[1]}" for col in hourly_features.columns[1:]]
  return hourly_features

# Load and preprocess the data
data = load_data("/content/sample_data/delhi.csv")
if data is not None:
  data = convert_time_columns(data)
  data = preprocess_data(data)

  # Print data summary before and after processing
  print("\nData Summary Before Processing:")
  print(data.describe())

  # Feature extraction for hourly data
  hourly_features = extract_hourly_features(data)

  # Print numeric summaries
  print("\nHourly Features Summary:")
  print(hourly_features.describe())

  # Handle missing values
  hourly_features = handle_missing_values(hourly_features)

  # Plot correlation matrix
  plot_correlation_matrix(hourly_features)

  # Define the target column for prediction
  target_column = 'tempC_mean'  # Change this to your target column as necessary
  if target_column not in hourly_features.columns:
      print(f"Error: Target column '{target_column}' not found in features DataFrame.")
  else:
      # Prepare features and target variable
      X = hourly_features.drop(columns=['date', target_column]).values  # All columns except 'date' and target
      y = hourly_features[target_column].values  # Target column

      # Split the data into training (80%) and testing (20%) sets
      train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=0)

      # Scale the features
      train_X, test_X = scale_data(train_X, test_X)

      # Define models for training and evaluation
      models = {
          "Linear Regression": LinearRegression(),
          "Ridge Regression": Ridge(alpha=1.0),
          "Lasso Regression": Lasso(alpha=1.0),
          "ElasticNet Regression": ElasticNet(alpha=1.0, l1_ratio=0.5),
          "Decision Tree Regressor": DecisionTreeRegressor(max_depth=10, random_state=0),
          "Random Forest Regressor": RandomForestRegressor(max_depth=10, n_estimators=100, random_state=0),
          "Gradient Boosting Regressor": GradientBoostingRegressor(n_estimators=100, max_depth=3, random_state=0),
          "XGBoost Regressor": XGBRegressor(n_estimators=100, max_depth=3, random_state=0),
          "LightGBM Regressor": LGBMRegressor(n_estimators=100, max_depth=3, min_data_in_bin=5, min_data_in_leaf=5, random_state=0),
          "CatBoost Regressor": CatBoostRegressor(iterations=100, depth=3, learning_rate=0.1, verbose=0),
          "K-Nearest Neighbors (KNN)": KNeighborsRegressor(n_neighbors=5),
          "Support Vector Regressor (SVR)": SVR(kernel='rbf', C=1.0, epsilon=0.2),
      }

      # Train and evaluate each model
      best_model_name = None
      best_model = None
      best_mae = float('inf')
      best_predictions = None

      for model_name, model in models.items():
          print(f"\nTraining {model_name}...")
          mae, mse, r2, predictions = train_and_evaluate_model(model, train_X, train_y, test_X, test_y, model_name)
          if mae < best_mae:
              best_mae = mae
              best_model_name = model_name
              best_model = model
              best_predictions = predictions

      # Summarize the best model
      summarize_best_model(best_model_name, best_model, train_X, train_y, test_X, test_y)

      # Display the best predictions
      print("\nBest Predictions from the Best Model:")
      best_predictions_df = pd.DataFrame({
          'True Values': test_y,
          'Predictions': best_predictions
      })
      print(best_predictions_df.head(10))  # Display the first 10 predictions

"""Extracting Daily Features: Create a function to aggregate the data daily.
Training and Evaluating Models: Use the daily features for training and evaluation.
Outputting Predictions: Display the predictions alongside the true values for the daily data.
"""

# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns  # For correlation heatmap
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from lightgbm import LGBMRegressor
from catboost import CatBoostRegressor

def load_data(file_path):
  """Load data from a CSV file."""
  try:
      data = pd.read_csv(file_path)
      print("Data loaded successfully.")
  except Exception as e:
      print(f"Error loading data: {e}")
      return None
  return data

def convert_time_columns(data):
  """Convert time columns from strings to a suitable datetime format."""
  time_columns = ['moonrise', 'moonset', 'sunrise', 'sunset']
  for col in time_columns:
      data[col] = pd.to_datetime(data[col], format='%I:%M %p', errors='coerce')
  return data

def preprocess_data(data):
  """Preprocess the data by converting date_time and numeric columns."""
  data['date_time'] = pd.to_datetime(data['date_time'], format='%Y-%m-%d %H:%M:%S', errors='coerce')
  data.dropna(subset=['date_time'], inplace=True)
  data.set_index('date_time', inplace=True)
  data.rename(columns={'uvIndex': 'uvIndex_1', 'uvIndex.1': 'uvIndex_2'}, inplace=True)

  # Define numeric columns for conversion
  numeric_columns = [
      'maxtempC', 'mintempC', 'totalSnow_cm', 'sunHour', 'uvIndex_1',
      'uvIndex_2', 'moon_illumination', 'DewPointC', 'FeelsLikeC',
      'HeatIndexC', 'WindChillC', 'WindGustKmph', 'cloudcover',
      'humidity', 'precipMM', 'pressure', 'tempC', 'visibility',
      'winddirDegree', 'windspeedKmph'
  ]

  # Convert columns to numeric and drop any rows with NaN values
  for col in numeric_columns:
      data[col] = pd.to_numeric(data[col], errors='coerce')

  data.dropna(inplace=True)
  return data

def extract_daily_features(data):
  """Extract daily features from the dataset."""
  # Resample to daily frequency and aggregate only numeric columns
  numeric_columns = data.select_dtypes(include=[np.number]).columns.tolist()
  daily_features = data.resample('D')[numeric_columns].agg(['mean', 'sum', 'max', 'min']).reset_index()
  daily_features.columns = ['date'] + [f"{col[0]}_{col[1]}" for col in daily_features.columns[1:]]
  return daily_features

def handle_missing_values(features):
  """Handle missing values in the features DataFrame by dropping them."""
  features.dropna(inplace=True)
  return features

def scale_data(train_X, test_X):
  """Scale the training and testing data using Min-Max scaling."""
  scaler = MinMaxScaler()
  train_X_scaled = scaler.fit_transform(train_X)
  test_X_scaled = scaler.transform(test_X)
  return train_X_scaled, test_X_scaled

def plot_results(y_true, y_pred, model_name):
  """Plot the true values vs predictions for model evaluation."""
  plt.figure(figsize=(12, 6))
  plt.plot(y_true, label='True Values', color='blue', alpha=0.7)
  plt.plot(y_pred, label='Predictions', color='red', alpha=0.7)
  plt.title(f'{model_name} Predictions vs True Values')
  plt.xlabel('Samples')
  plt.ylabel('Values')
  plt.legend()
  plt.show()

def plot_correlation_matrix(features):
  """Plot the correlation matrix as a heatmap."""
  plt.figure(figsize=(12, 10))
  correlation_matrix = features.corr()
  sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='coolwarm', square=True)
  plt.title('Correlation Matrix')
  plt.show()

def train_and_evaluate_model(model, train_X, train_y, test_X, test_y, model_name):
  """Train the model and evaluate its performance using MAE, MSE, and R² metrics."""
  model.fit(train_X, train_y)
  predictions = model.predict(test_X)
  mae = mean_absolute_error(test_y, predictions)
  mse = mean_squared_error(test_y, predictions)
  r2 = r2_score(test_y, predictions)
  print(f"{model_name} MAE: {mae:.4f}, MSE: {mse:.4f}, R²: {r2:.4f}")
  plot_results(test_y, predictions, model_name)
  return mae, mse, r2, predictions

def summarize_best_model(best_model_name, best_model, train_X, train_y, test_X, test_y):
  """Summarize the best model's performance and its benefits."""
  print(f"\nSummary of the Best Model: {best_model_name}")
  predictions = best_model.predict(test_X)
  mae = mean_absolute_error(test_y, predictions)
  mse = mean_squared_error(test_y, predictions)
  r2 = r2_score(test_y, predictions)

  print(f"Mean Absolute Error (MAE): {mae:.4f}")
  print(f"Mean Squared Error (MSE): {mse:.4f}")
  print(f"R² Score: {r2:.4f}")

  # Display the best predictions with model name
  print("\nBest Predictions from the Best Model:")
  best_predictions_df = pd.DataFrame({
      'Model': [best_model_name] * len(test_y),  # Add model name for each prediction
      'True Values': test_y,
      'Predictions': predictions
  })
  print(best_predictions_df.head(10))  # Display the first 10 predictions

# Load and preprocess the data
data = load_data("/content/sample_data/delhi.csv")  # Ensure the correct path to the uploaded file
if data is not None:
  data = convert_time_columns(data)
  data = preprocess_data(data)

  # Print data summary before and after processing
  print("\nData Summary Before Processing:")
  print(data.describe())

  # Feature extraction for daily data
  daily_features = extract_daily_features(data)

  # Print numeric summaries
  print("\nDaily Features Summary:")
  print(daily_features.describe())

  # Handle missing values
  daily_features = handle_missing_values(daily_features)

  # Plot correlation matrix
  plot_correlation_matrix(daily_features)

  # Define the target column for prediction
  target_column = 'tempC_mean'  # Change this to your target column as necessary
  if target_column not in daily_features.columns:
      print(f"Error: Target column '{target_column}' not found in features DataFrame.")
  else:
      # Prepare features and target variable
      X = daily_features.drop(columns=['date', target_column]).values  # All columns except 'date' and target
      y = daily_features[target_column].values  # Target column

      # Split the data into training (80%) and testing (20%) sets
      train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=0)

      # Scale the features
      train_X, test_X = scale_data(train_X, test_X)

      # Define models for training and evaluation
      models = {
          "Linear Regression": LinearRegression(),
          "Ridge Regression": Ridge(alpha=1.0),
          "Lasso Regression": Lasso(alpha=1.0),
          "ElasticNet Regression": ElasticNet(alpha=1.0, l1_ratio=0.5),
          "Decision Tree Regressor": DecisionTreeRegressor(max_depth=10, random_state=0),
          "Random Forest Regressor": RandomForestRegressor(max_depth=10, n_estimators=100, random_state=0),
          "Gradient Boosting Regressor": GradientBoostingRegressor(n_estimators=100, max_depth=3, random_state=0),
          "XGBoost Regressor": XGBRegressor(n_estimators=100, max_depth=3, random_state=0),
          "LightGBM Regressor": LGBMRegressor(n_estimators=100, max_depth=3, min_data_in_bin=5, min_data_in_leaf=5, random_state=0),
          "CatBoost Regressor": CatBoostRegressor(iterations=100, depth=3, learning_rate=0.1, verbose=0),
          "K-Nearest Neighbors (KNN)": KNeighborsRegressor(n_neighbors=5),
          "Support Vector Regressor (SVR)": SVR(kernel='rbf', C=1.0, epsilon=0.2),
      }

      # Train and evaluate each model
      best_model_name = None
      best_model = None
      best_mae = float('inf')
      best_predictions = None

      for model_name, model in models.items():
          print(f"\nTraining {model_name}...")
          mae, mse, r2, predictions = train_and_evaluate_model(model, train_X, train_y, test_X, test_y, model_name)
          if mae < best_mae:
              best_mae = mae
              best_model_name = model_name
              best_model = model
              best_predictions = predictions

      # Summarize the best model
      summarize_best_model(best_model_name, best_model, train_X, train_y, test_X, test_y)

      # Display the best predictions
      print("\nBest Predictions from the Best Model:")
      best_predictions_df = pd.DataFrame({
          'True Values': test_y,
          'Predictions': best_predictions
      })
      print(best_predictions_df.head(10))  # Display the first 10 predictions